{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "sys.path.append(\"../tools/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções definidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_RetirarNaNs(data_dict):\n",
    "    '''\n",
    "    Retira os NaNs \n",
    "    data_dict: dicionário com diversas features para cada pessoa\n",
    "    return: dicionário de entrada, exceto as features que possuem valor NaN\n",
    "    '''\n",
    "    \n",
    "    for k1, v1 in data_dict.iteritems():\n",
    "        data_dict[k1] = {key:value for key, value in v1.iteritems() if value != 'NaN'}\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_FeatureScalling(data_dict, not_scale, features):\n",
    "    '''\n",
    "    Cria nova escala para features de cada pessoa no dicionário de entrada\n",
    "    data_dict: dicionário com diversas features para cada pessoa\n",
    "    not_scale: features que não devem ser alteradas\n",
    "    return: tuple com features do dicionário em nova escala e um dicionário com o conversor de escalas\n",
    "    '''\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    #Criar escala para cada feature\n",
    "    scale_feature = dict()\n",
    "    for feature in features: \n",
    "        if not feature in not_scale:\n",
    "            scale_feature[feature] = MinMaxScaler()\n",
    "            scale_feature[feature].fit_transform(np.array([data_dict[key][feature]\n",
    "                                                for key in data_dict.keys()\n",
    "                                                if feature in data_dict[key].keys()]).reshape(-1,1))\n",
    "    \n",
    "    #Aplicar escala sobre os campos de interesse\n",
    "    for k1, v1 in data_dict.iteritems():\n",
    "        for key, value in v1.iteritems():\n",
    "            if key in features and not key in not_scale:\n",
    "                data_dict[k1][key] = scale_feature[key].transform(np.array(value).reshape(-1,1))[0][0]\n",
    "            else:\n",
    "                data_dict[k1][key] = value\n",
    "    \n",
    "    return data_dict, scale_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_OrdemFeatures(data_dict, features):\n",
    "    '''\n",
    "    Retorna uma lista com as features ordenadas por valor\n",
    "    data_dict: dicionários com as features\n",
    "    features: features a serem ordenadas\n",
    "    return: lista com ordem das features\n",
    "    '''\n",
    "    \n",
    "    ordem = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        \n",
    "        aux = {keys: values[feature] for keys, values in data_dict.items() if feature in values.keys()}\n",
    "        \n",
    "        ordem[feature] = sorted(aux.keys(), key=lambda x:data_dict[x][feature])\n",
    "        \n",
    "    return ordem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_PlotOutliers(test_list, ordem):\n",
    "    '''\n",
    "    Criar gráfico com valores ordenados\n",
    "    test_list: lista com os dados\n",
    "    ordem: ordem dos gráficos\n",
    "    return: maiores valores em cada gráfico (análise de outliers)\n",
    "    '''\n",
    "    \n",
    "    outliers_dict = {}\n",
    "    n_dados = len(test_list)\n",
    "    \n",
    "    f, gp = plt.subplots(n_dados,1, sharex= True)\n",
    "\n",
    "    f.set_figheight((2.5*n_dados)//2+1)\n",
    "    f.set_figwidth(15)\n",
    "\n",
    "    for jj, feature in enumerate(test_list):\n",
    "        max_value = 0\n",
    "        person = ''\n",
    "        \n",
    "        try:\n",
    "\n",
    "            for ii, point in enumerate(ordem[feature]):\n",
    "                if feature in data_dict[point]:\n",
    "\n",
    "                    salary = data_dict[point][feature]\n",
    "\n",
    "                    if max_value < salary:\n",
    "                        max_value = salary\n",
    "                        person = point\n",
    "\n",
    "                    #Registrando os valores extremos encontrados\n",
    "                    outliers_dict[feature] = [max_value, person]\n",
    "\n",
    "                    gp[jj].scatter(ii, salary)\n",
    "\n",
    "                gp[jj].set_title(feature)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_PessoaEmail(dir_, address):\n",
    "    \n",
    "    '''\n",
    "    Procura no repositório dir_ o endereço de destino contido no email\n",
    "    dir_: diretório dos emails\n",
    "    address: email procurado\n",
    "    '''\n",
    "    lista = []\n",
    "    if email in address.keys():\n",
    "        with open('{}/{}'.format(dir_, address[email], 'r')) as fl:\n",
    "            lista = [line.split('/')[2] for line in fl]\n",
    "    \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_ExtraiPoisAbreviados(data_dict):\n",
    "    '''\n",
    "    Extrair os POis e abrevia os nomes pela regra de sobrenome-Nome(primeira letra), exemplo, Kenneth Lee Lay para lay-k\n",
    "    data_dict: dicionário com dados das pessoas\n",
    "    return: dictionary com nomes convertidos por pessoa\n",
    "    '''\n",
    "\n",
    "    aux_list = {}\n",
    "    for ii in [key for key, value in data_dict.items() if value['poi']==True]:\n",
    "        string = ii.split(' ')\n",
    "        name = string[0]\n",
    "\n",
    "        second_name = string[1][0] if len(string) in [3,2] else string[2][0]\n",
    "        \n",
    "        try:\n",
    "            aux_list[data_dict[ii]['email_address']] = '-'.join([name, second_name]).lower()\n",
    "        except:\n",
    "            print 'Erro na chave {key}'.format(key=ii)\n",
    "        \n",
    "    return aux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_StatMsg(email, aux_list):\n",
    "    '''\n",
    "    Retorna a quantidade de e-mails trocados com POis\n",
    "    email: dictionary com os destinos dos emails enviados por uma pessoa\n",
    "    return: dictionary com contagem do total de emails enviados para POis para cada indivíduo\n",
    "    '''\n",
    "    \n",
    "    count, ratio = {}, {}\n",
    "    for key in email.keys():\n",
    "        \n",
    "        aux = aux_list.copy()\n",
    "        if key in aux.keys():\n",
    "        #Não contar a própria pessoa -- acontece quando a pessoa é um poi\n",
    "            aux.pop(key)\n",
    "        \n",
    "        #Somente os que são poi\n",
    "        lista = [ii for ii in email[key] if ii in aux.values()]\n",
    "        \n",
    "        #total de poi com troca de mensagens\n",
    "        count[key] = len(set(lista))\n",
    "        \n",
    "        #razao entre mensagens com poi e total de mensagens\n",
    "        if len(email[key])>0:\n",
    "            ratio[key] = len(lista)/float(len(email[key]))\n",
    "        else:\n",
    "            ratio[key]= 0\n",
    "    \n",
    "    return count, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_VerificarExisteFeature():\n",
    "    '''Verificar se existe chave sem alguma característica \n",
    "        return: key e feature faltante, além da quantidade de keys que precisam ser corrigidas\n",
    "    '''\n",
    "    \n",
    "    person_feature = defaultdict(list)\n",
    "    \n",
    "    all_features = {key for key, value in my_dataset.items() if all(ii == features_list for ii in value.keys())}\n",
    "    all_persons = {key for key in my_dataset.keys()}\n",
    "    \n",
    "    #procura as pessoas que não tem todas as features e encontra a features faltante\n",
    "    for jj in all_persons - all_features:\n",
    "        for ii in features_list:\n",
    "            if not ii in my_dataset[jj].keys():\n",
    "                person_feature[jj].append(ii)\n",
    "    \n",
    "    print 'Quantidade de pessoas com falta de informações, ', len(person_feature)\n",
    "    return person_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','total_payments', 'bonus', 'total_stock_value', 'expenses', \n",
    "                 'exercised_stock_options', 'restricted_stock','shared_receipt_with_poi','ratio_to','ratio_from'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "Temos que alguns campos estão como 'NaN', logo estes campos não serão considerados neste momento para facilitar a análise de outliers e outras análises que a falta de dados pode enviesar. Se os campos com 'NaN' estiverem na feature_list eles serão futuramente adicionados com valor 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de retirar os valores NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['METTS MARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = F_RetirarNaNs(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após ser retirado os NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['METTS MARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pasta email_by_address será utilizada nesse trabalho. Como algumas keys em data_dict não possuem e-mail, essas pessoas serão desconsideradas. Isso reduz a base de dados, mas trará maior consistência do que supor que essas não trocam e-mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    if not 'email_address' in data_dict[key].keys():\n",
    "        data_dict.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Remove outliers\n",
    "A análise gráfico será utilizada para descobrir se existem outliers na base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = ['salary','total_payments', 'bonus', 'total_stock_value', 'expenses', 'exercised_stock_options', \n",
    "             'restricted_stock', 'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "ordem = F_OrdemFeatures(data_dict, plot_list)\n",
    "F_PlotOutliers(plot_list, ordem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos que os valores extremos nos gráficos estão a pessoal com altos cargos na Enron, como:\n",
    "\n",
    "**LAVORATO JOHN J**: Principais executivos\n",
    "\n",
    "**LAY KENNETH L**: fundador e CEO da companhia na época do escândalo\n",
    "\n",
    "**SKILLING JEFFREY K**: fundador e ex-CEO da companhia na época do escândalo\n",
    "\n",
    "Essas pessoas são chefes da companhia, assim possuem salários e bonificações diferenciadas dos demais funcionários.\n",
    "\n",
    "*É necessário analisar os outros pontos que podem ser outleirs para determinar se também se tratam de pessoas com altos cargos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SALARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem['salary'][-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pontos que poderiam ser outliers são de chefes da companhia. Nesse caso temos que além de **LAY KENNETH L** e **SKILLING JEFFREY K** temos **FREVERT MARK A** que era na época CEO de uma subsidiára do grupo a _Enron Whosale Services_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISED STOCK OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem['exercised_stock_options'][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exercised_stock_options 2 altos executivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESTRICTED STOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem['restricted_stock'][-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso de restricted stock temos pessoas com altos cargos que recebiam remuneração através de ações associadas a desempenho. \n",
    "\n",
    "Temos que as 3 pessoas são altos executivos o que justifica os altos ganhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantidade de emails pode variar muito independente da pessoa e por isso não será considerada um outlier os valores extremos no gráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o cleaning data não existem outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 --  Create New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando os dados contidos em email_by_address vou criar duas features que trazem a quantidade de POis que uma pessoa se relacionou enviando ou recebendo emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Para cada pessoa em data_dict pego os destinatários dos emails from e to\n",
    "\n",
    "email_poi = ((data_dict[key]['email_address'], data_dict[key]['poi']) for key in data_dict.keys() \n",
    "                if 'email_address' in data_dict[key] and 'poi' in data_dict[key])\n",
    "\n",
    "address_from = {file_.split('_')[1][:-4]:file_ for file_ in os.listdir('emails_by_address') if file_.split('_')[0]=='from'}\n",
    "address_to = {file_.split('_')[1][:-4]:file_ for file_ in os.listdir('emails_by_address') if file_.split('_')[0]=='to'}\n",
    "\n",
    "from_, to_ = {}, {}\n",
    "for email, poi in email_poi:\n",
    "    from_[email] = F_PessoaEmail('emails_by_address', address_from)\n",
    "    to_[email] = F_PessoaEmail('emails_by_address', address_to)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_aux_list = F_ExtraiPoisAbreviados(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_aux_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar a quantidade POis com que a pessoa interage por e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from_count, from_ratio = F_StatMsg(from_, poi_aux_list)\n",
    "to_count, to_ratio = F_StatMsg(to_, poi_aux_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui tenho as novas características construídas, preciso adicionar ao data_dict\n",
    "\n",
    "from_ratio e to_ratio não serão incluídos, pois existem os campos from_messages, from_poi_to_this_person, from_this_person_to_poi e to_messages que podem ser combinados para gerar uma feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    \n",
    "    email = data_dict[key]['email_address']\n",
    "    \n",
    "    data_dict[key]['from_count'] = from_count[email]\n",
    "    data_dict[key]['to_count'] = to_count[email]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando razao de envio de e-mail\n",
    "As pessoas que não possuem algum campo de mensagem serão excluídas da base para manter a consistência. O outro caminho seria considerar como sendo 0 os campos de mensagens, mas prefiro nesse caso manter a consistências das informações a supor um valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[data_dict.keys()[27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in data_dict.items():\n",
    "    try:\n",
    "        \n",
    "        if value['to_messages'] == 0:\n",
    "            value['ratio_to'] = 0\n",
    "        \n",
    "        if value['from_messages'] == 0:\n",
    "            value['ratio_from'] = 0 \n",
    "        \n",
    "        if not 0 in [value['to_messages'],value['from_messages']]:\n",
    "            value['ratio_to'] = value['from_this_person_to_poi'] / float(value['to_messages'])\n",
    "            value['ratio_from'] = value['from_poi_to_this_person'] / float(value['from_messages'])\n",
    "            \n",
    "        value.pop('from_this_person_to_poi')\n",
    "        value.pop('to_messages')\n",
    "        value.pop('from_poi_to_this_person')\n",
    "        value.pop('from_messages')\n",
    "            \n",
    "        \n",
    "    except:\n",
    "        #se o campo não existe, não considero na base final\n",
    "        data_dict.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados está reduzida devido a desconsiderar as keys em data_dict que não possuem o campo e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[data_dict.keys()[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scalling\n",
    "Para evitar erros computacionais os valores devem ser bem condicionados, logo vou deixar os dados entre 0 e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi','salary','total_payments', 'bonus', 'total_stock_value', 'expenses', \n",
    "                 'exercised_stock_options', 'restricted_stock',\n",
    "                 'email_address', 'shared_receipt_with_poi', 'to_count', 'from_count','ratio_to','ratio_from'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['METTS MARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, scale  = F_FeatureScalling(data_dict, ['email_address','poi'], features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['METTS MARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso termino de fazer as correções da base e mudo a escala dos dados para fazer para facilitar os cáclulos numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrigindo a base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando se todas as keys em data_dict possuem todas as features em features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "not_person_feature = F_VerificarExisteFeature()\n",
    "not_person_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para não reduzir ainda mais a base de dados, vou adicionar as features em features_list em todas as keys contidas em data_dict para que seja possível gerar o numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, values in not_person_feature.items():\n",
    "    for value in values:\n",
    "        my_dataset[key][value] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando novamente se existe algum campo com problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "F_VerificarExisteFeature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessário retirar email_address dos campos de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_validas = features_list[:]\n",
    "features_validas.remove('email_address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrair features e labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_validas, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Try a varity of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando função para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifier Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=100, kernel='rbf', gamma=10)\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(1,),solver='lbfgs', learning_rate_init =.1)\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='gini', min_samples_split=3, max_depth=2,splitter='random')\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(criterion='entropy', max_features='log2',warm_start =True)\n",
    "clf.fit(features_train, labels_train)\n",
    "clf.score(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(clf, my_dataset, features_validas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor classificador foi o Random Forest. Devido a pequena quantidade de dados os classificadores testados tem uma grande de chance de sofrer Overfitting. Nos testes encontramos que Random Forest se adaptou melhor a pequena quantidade de dados por ter sua resposta baseado em vários classificadores fracos que não sofrem tanto com overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_validas)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
